[
  {
    "objectID": "mr_1.html",
    "href": "mr_1.html",
    "title": "\n2  Introduction to multiple regression\n",
    "section": "",
    "text": "2.0.1 Load packages\nlibrary(tidyverse)  # tidy and wrangle data\nlibrary(broom)      # create tidy tables for statistical objects\nlibrary(effectsize) # calculate effect size\nlibrary(kableExtra) # create tidy tables\nlibrary(pscl)\nlibrary(faux)       # simulate data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to multiple regression</span>"
    ]
  },
  {
    "objectID": "mr_1.html#multiple-linear-regression",
    "href": "mr_1.html#multiple-linear-regression",
    "title": "\n2  Introduction to multiple regression\n",
    "section": "\n2.1 Multiple linear regression",
    "text": "2.1 Multiple linear regression\nIn simple linear regression, we model the relationship between one predictor and one outcome, such as predicting exam score from hours studied, while in multiple linear regression, we use two or more predictors (which can be continuous or categorical) to explain variation in the outcome. This allows us to estimate the unique effect of each predictor while controlling for others, and it also makes it possible to include interaction terms, where the effect of one predictor (e.g., a continuous variable like hours studied) depends on the level of another predictor (e.g., a categorical variable like teaching method).\nWith two predictor variables (\\(x_1\\)) and (\\(x_2\\)) the prediction of \\(y\\) is expressed by the following equation:\n\\[\ny = \\beta_0 + \\beta_{1}X_{1} + \\beta_{2}X_{2} + e\n\\]\nwhere the \\(\\beta_1\\) and \\(\\beta_2\\) measure the association between \\(x_1\\) and \\(x_2\\) and \\(y\\). Specifically, \\(\\beta_1\\) is interpreted as the average effect of a one unit increase in \\(x_1\\) on \\(y\\), holding the other predictors constant, and \\(\\beta_2\\) is interpreted similarly for \\(x_2\\).\nWhen adding a second predictor (\\(x_2\\)):\nIf \\(x_2\\) is a continuous predictor (e.g., nclicks), the corresponding regression coefficient (b1) is a slope, interpreted as the difference in the mean of y associated with a one-unit difference in x when holding all other predictors fixed (or “controlling for” or “adjusted for” the other predictors).\nIf \\(x_2\\) is a categorical predictor (e.g., sex) with L levels, then instead of just one corresponding \\(x\\) term in the model, there are L-1 indicator variables. Each of the L-1 corresponding regression coefficients is interpreted as the difference between the mean of \\(y\\) at that level of \\(x\\) and the mean of \\(y\\) at the reference level, when holding all other predictors fixed (or “controlling for” or “adjusted for” the other predictors).\nWhen we talk about “controlling for” or “adjusted for” what we are saying is that by adding a second predictor (x2) in the model, we statistically hold the values of x2 constant. That is to say:\n\nNo changes in \\(y\\) can be attributed to changes in \\(x_2\\)\nSince the value of \\(x_2\\) is being held constant, the only remaining source of change in \\(y\\) is a change in \\(x_1\\)\n\n\n2.1.1 Model with two continuous predictors\nSuppose we want to examine how GPA and access to Canvas affect grades, we can fit a multiple regression model using the data set:\n\nhead(grades, n = 5)\n\n     grade       GPA lecture nclicks\n1 2.395139 1.1341088       6      88\n2 3.665159 0.9708817       6      96\n3 2.852750 3.3432350       6     123\n4 1.359700 2.7595763       9      99\n5 2.313335 1.0222222       4      66\n\n\nLet’s fit a regression model with the predictors GPA and nclicks:\n\nmodel2 &lt;- lm(grade ~ GPA + nclicks, grades) |&gt; \n  tidy()\n\nmodel2\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  1.52      0.564       2.69  0.00842\n2 GPA          0.219     0.102       2.15  0.0344 \n3 nclicks      0.00564   0.00590     0.957 0.341  \n\n\nGiven the output, our regression model equation with two predictors can be written as follow:\n\ngrade = 1.52 + 0.22*GPA + 0.01*nclicks\n\ngrade increases by 0.22 as GPA increases by one point while holiding nclicks constant. The regression coefficient of GPA is statistically significant (p = 0.0343807) indicating that is statistically different from 0. The regression coefficient of nclicks term has a value of 0.01. grade increases by 0.0056 as nclicks` increases by 1 while holding GPA constant. Unlike GPA, the regression coefficient of nclicks is not statistically significant (p = 0.341085) indicating that is not statistically different from 0.\nWhen we say “holding nclicks constant”, we mean that the effect of GPA is estimated while comparing student with the same number of clicks. For instance, suppose we want to compare two students with the same number of clicks (i.e., holding nclicks constant):\nBoth students have 50 clicks, but different GPAs:\n\n\nStudent A: GPA = 3\n\nround(model2[1,2] + model2[2,2]*3 + model2[3,2]*50, 2) |&gt; \n  pull()\n\n[1] 2.46\n\n\n\n\nStudent B: GPA = 4\n\nround(model2[1,2] + model2[2,2]*4 + model2[3,2]*50, 2) |&gt; \n  pull()\n\n[1] 2.68\n\n\n\n\nThe difference in grade between both students is the effect of a 1-point GPA increase at the same number of clicks. That’s all “holding constant” means: compare outcomes at the same value of the other variable.\n\n2.1.2 Model with one continuous and one categorical predictor\n\n# create a categotical variable\ngrades$sport &lt;- as.factor(rep(c(\"team\", \n                                \"individual\"), \n                                each = 50))\n\nhead(grades)\n\n     grade       GPA lecture nclicks sport\n1 2.395139 1.1341088       6      88  team\n2 3.665159 0.9708817       6      96  team\n3 2.852750 3.3432350       6     123  team\n4 1.359700 2.7595763       9      99  team\n5 2.313335 1.0222222       4      66  team\n6 2.582859 0.8413324       8      99  team\n\n\nThe variable sport is categorical with two levels (team and individual). Now suppose that we want to determine whether lecture and sport affect grade:\n\nlm(grade ~ lecture + sport, grades) |&gt; \n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n1.914\n0.323\n5.930\n0.000\n\n\nlecture\n0.105\n0.043\n2.467\n0.015\n\n\nsportteam\n-0.105\n0.174\n-0.602\n0.548\n\n\n\n\n\nIn the fitted model, teamis used as the reference category. As a result, the model includes one coefficient representing the difference in the outcome variable between team and individual. The intercept corresponds to the expected value of grade when x is the student plays an individual sport.\nWhen using regression models, by default R treats the first level of a categorical factor (alphabetically or numerically) as the reference level in regression models . This is why indiviudal is treated as the reference level. This default behavior can be changed using three methods.\n\nDummy coding\n\nDummy coding is the process of converting categorical variables into numeric variables (e.g., 0, 1, 2) so they can be included in regression models. This allows the model to estimate differences between categories.\nFor example, suppose we want to include the variable sport (team vs. individual) in a regression. We can create a dummy variable as follows:\n\ngrades_dummy &lt;- grades |&gt; \n  mutate(sport_dummy = as.factor(if_else(sport == \"team\", 0, 1)))\n\nhead(grades_dummy, 5)\n\n     grade       GPA lecture nclicks sport sport_dummy\n1 2.395139 1.1341088       6      88  team           0\n2 3.665159 0.9708817       6      96  team           0\n3 2.852750 3.3432350       6     123  team           0\n4 1.359700 2.7595763       9      99  team           0\n5 2.313335 1.0222222       4      66  team           0\n\n\nHere team is coded as 0 and will be the reference category (intercept ) whereas individual is coded as 1 and its coefficient in the regression will represent the difference from the reference category. Let’s fit the model:\n\nlm(grade ~ GPA + sport, grades_dummy) |&gt; \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   2.05      0.269      7.63  1.63e-11\n2 GPA           0.246     0.0978     2.52  1.35e- 2\n3 sportteam    -0.0835    0.174     -0.480 6.33e- 1\n\n\nNow the reference level is team because it corresponds to the first level of the factor, which in this case is 0. By convention, the reference level is often assigned the value 0 when encoding categorical variables numerically.\n\nUsing factor()\n\n\nWe can reorder the levels of the factor to specify which level we want to the the reference level.\n\ngrades$sport &lt;- factor(\n  grades$sport,\n  levels = c(\"team\", \"individual\")\n)\n\nWe then fit the model:\n\nlm(grade ~ GPA + sport, grades) |&gt; \n  tidy() \n\n# A tibble: 3 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       1.97      0.262      7.51  2.93e-11\n2 GPA               0.246     0.0978     2.52  1.35e- 2\n3 sportindividual   0.0835    0.174      0.480 6.33e- 1\n\n\n\nUsing relevel()\n\n\nrelevel() allows you to change the reference level without having to reorder the entire factor.\n\ngrades$sport &lt;- relevel(grades$sport, ref = \"team\")\n\nWe fit the model again:\n\nlm(grade ~ GPA + sport, grades) |&gt; \n   tidy()\n\n# A tibble: 3 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       1.97      0.262      7.51  2.93e-11\n2 GPA               0.246     0.0978     2.52  1.35e- 2\n3 sportindividual   0.0835    0.174      0.480 6.33e- 1\n\n\n\n2.1.3 Model with an interaction effect between two predictors\nAn interaction effect occurs when the effect of one predictor variable on the outcome depends on the level or value of another predictor. In other words, the relationship between \\(y\\) and \\(x_1\\) changes depending on \\(x_2\\)\nThe model with interaction effects between two predictors can be written as follows:\n\\[\ny = b_0 + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{1}X_{2} + e\n\\]\nIn the interaction model, if b3 is zero, the equation reduces to the equation for the main-effects model, namely:\n\\[\ny = b_0 + b_{1}X_{1} + b_{2}X_{2} + e\n\\]\n\n2.1.4 Interaction between a two categorical predictors\nSuppose we want to determine whether the efficacy of a new drug to reduce blood pressure (BP) is improved by the presence of a second moderator or predictor. Specifically, we are interested in whether the efficacy of the drug to reduce blood pressure is higher in patients that do exercise in comparison to those who are sedentary. This would be an example of a study where the effect of interest is an interaction effect.\nExample 1\nLet’s simulate data for a 2 x 2 factorial design\n\n# For reproducibility\nset.seed(040404)\n\n# Define the design\ndesign &lt;- list(\n  treatment = c(\"placebo\", \"drug\"),\n  sport = c(\"yes\", \"no\")\n)\n\n# Define the means for each condition\nmu &lt;- c(57, 60, 37, 41)\n\n# Simulate the data\nsim_data &lt;- sim_design(\n  between = design,\n  dv = \"BP\",\n  mu = mu,\n  n = 30,  # Number of participants per condition\n  sd = 5,  # Standard deviation\n  long = TRUE,\n  plot = FALSE\n)\n\nPlot the simulated data:\n\nggplot(sim_data, aes(x = treatment, y = BP, group = sport, color = sport)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +  \n  coord_cartesian(ylim = c(30, 80)) +\n  labs(y = \"Blood pressure\", x = \"Treatment\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 2.1: No interaction effect\n\n\n\n\nFit the model with an interaction effect:\n\nres1 &lt;- lm(BP ~ treatment * sport, data = sim_data) |&gt; \n  tidy() |&gt; \n  mutate(\n    p_value = as.numeric(formatC(p.value, format = \"f\", digits = 4)\n  )) |&gt; \n  select(-p.value)\n\nres1\n\n# A tibble: 4 × 5\n  term                  estimate std.error statistic p_value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)             58.4       0.847   68.9      0    \n2 treatmentdrug          -21.5       1.20   -18.0      0    \n3 sportno                  2.45      1.20     2.05     0.043\n4 treatmentdrug:sportno    0.135     1.69     0.0799   0.936\n\n\nThe regression model shows that BP for the reference group (placebo and sport = yes) is 58.37. When sport = yes, taking the drug reduces BP by approximately -21.5units, a highly significant effect. Being in sport = no significantly increases BP (2.45) in the placebo group. The interaction between treatment and sport (0.14) indicates that the drug’s effect is slightly smaller for sport = no, but this difference is not statistically significant. Overall, the drug substantially reduces BP regardless of sport participation, and there is no meaningful interaction, so the effect of the drug is similar across both sport groups.\nExample 2\n\n# For reproducibility\nset.seed(123) \n\n# Define the design\ndesign &lt;- list(\n  treatment = c(\"placebo\", \"drug\"),\n  sport = c(\"yes\", \"no\")\n)\n\n# Define the means for each condition\nmu &lt;- c(38, 40, 38, 40)\n\n# Simulate the data\nsim_data2 &lt;- sim_design(\n  between = design,\n  dv = \"BP\",\n  mu = mu,\n  n = 30,  # Number of participants per condition\n  sd = 5,  # Standard deviation\n  long = TRUE,\n  plot = FALSE\n)\n\n\nggplot(sim_data2, aes(x = treatment, y = BP, group = sport, color = sport)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  coord_cartesian(ylim = c(30, 50)) +\n  labs(y = \"Mood\", x = \"Treatment\")\n\n\n\n\n\n\nFigure 2.2: No interaction effect\n\n\n\n\n\nres2 &lt;- lm(BP ~ treatment * sport, data = sim_data) |&gt; \n  tidy() |&gt; \n  mutate(\n    p_value = as.numeric(formatC(p.value, format = \"f\", digits = 4)\n  )) |&gt; \n  select(-p.value)\n\nres2\n\n# A tibble: 4 × 5\n  term                  estimate std.error statistic p_value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)             58.4       0.847   68.9      0    \n2 treatmentdrug          -21.5       1.20   -18.0      0    \n3 sportno                  2.45      1.20     2.05     0.043\n4 treatmentdrug:sportno    0.135     1.69     0.0799   0.936\n\n\nExample 3\n\n# For reproducibility\nset.seed(123) \n\n# Define the design\ndesign &lt;- list(\n  treatment = c(\"placebo\", \"drug\"),\n  sport = c(\"yes\", \"no\")\n)\n\n# Define the means for each condition\nmu &lt;- c(55, 62, 40, 55)\n\n# Simulate the data\nsim_data3 &lt;- sim_design(\n  between = design,\n  dv = \"BP\",\n  mu = mu,\n  n = 30,  # Number of participants per condition\n  sd = 5,  # Standard deviation\n  long = TRUE,\n  plot = FALSE\n)\n\nLet’s plot the data\n\nggplot(sim_data3, aes(x = treatment, y = BP, group = sport, color = sport)) +\n  stat_summary(fun = mean, geom = \"line\", size = 1.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) + \n  coord_cartesian(ylim = c(30, 80)) +\n  labs(y = \"Blood pressure\", x = \"Treatment\")\n\n\n\n\n\n\nFigure 2.3: Interaction effect\n\n\n\n\nLet’s fit the model\n\nmodel &lt;- lm(BP ~ treatment * sport, data = sim_data3) \n\nmodel|&gt; tidy()\n\n# A tibble: 4 × 5\n  term                  estimate std.error statistic  p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)              54.8      0.821     66.7  2.45e-94\n2 treatmentdrug           -14.6      1.16     -12.6  1.80e-23\n3 sportno                   8.13     1.16       7.00 1.81e-10\n4 treatmentdrug:sportno     6.28     1.64       3.82 2.13e- 4\n\n\nHow to interpret this output:\n\n(Intercept) = 54.8: This is the estimated mean blood pressure for subjects in the placebo group who are active.\n\ntreatmentdrug = -14.6: This represents the change in blood pressure for subjects who are in the drug treatment and are active. Their blood pressure is:\n54.8 + -14.6 =\n40.1\n\n\nsportno = 8.1: This represents the change in blood pressure for subjects in placebo group and are sedentary. Their blood pressure is:\n54.8 + 8.1 =\n62.9\n\n\ntreatmentdrug:sportno = 6.3: This is the interaction effect: the additional change in blood pressure for subjects who are sedentary and in the drug group. So, their expected blood pressure is:\n54.8 + (-14.6) + 8.1 + 6.3 =\n54.5\n\n\nAn interaction effect occurs when the effect of one predictor varies depending on the level of another predictor. Graphically, this is shown when the lines representing different groups are not parallel. Conversely, parallel lines typically indicate the absence of an interaction, meaning the effect of one predictor is consistent across the levels of the other.\n\n2.1.5 Interaction between a continuous and categorical predictors\nSuppose we want to determine whether the effect of GPA on grade differs between students who play “team sports” and those who play “individual sports”. Specifically, we aim to compare whether the effect of GPA on grade varies as a function of sport.\n\nggplot(grades, aes(y = grade, x = GPA, colour = sport)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nHere we see very clearly the difference in slope that we built into our data. How do we test whether the two slopes are significantly different? To do this, we can’t have two separate regressions. We need to bring the two regression lines into the same model. How do we do this?\n\nlm(grade ~ GPA + sport + GPA*sport, grades) |&gt; \n  tidy() |&gt; \n  mutate(p.value = format.pval(p.value, digits = 3, scientific = FALSE)) |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n2.0187700\n0.3520472\n5.7343736\n0.000000113\n\n\nGPA\n0.2234913\n0.1394717\n1.6024138\n0.112\n\n\nsportindividual\n-0.0241565\n0.5034438\n-0.0479826\n0.962\n\n\nGPA:sportindividual\n0.0448184\n0.1965006\n0.2280829\n0.820",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to multiple regression</span>"
    ]
  },
  {
    "objectID": "mr_2.html",
    "href": "mr_2.html",
    "title": "\n3  Multiple regression 2 (Lecture 2)\n",
    "section": "",
    "text": "3.1 Model comparison\nLoad packages\nLoad data\nFor learning purposes, we will add two new variables: sport, which is categorical, and study_time which is numerical.\nThe overall quality of a model can be assessed by examining R2 and by comparing models using anova().\nLet’s first fit four models:\n# null modell (no predictor, only y-intercept)\nmodel0 &lt;- lm(grade ~ 1, grades)\n\n# model with one predictor\nmodel1 &lt;- lm(grade ~ study_time, grades) \n\n#model with two predictors\nmodel2 &lt;- lm(grade ~ study_time + GPA, grades) \n\n# model with three predictors\nmodel3 &lt;- lm(grade ~ study_time + GPA + lecture, grades) \n\n# model with four predictors\nmodel4 &lt;- lm(grade ~ study_time + GPA + lecture + nclicks, grades)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression 2 (Lecture 2)</span>"
    ]
  },
  {
    "objectID": "mr_2.html#model-comparison",
    "href": "mr_2.html#model-comparison",
    "title": "\n3  Multiple regression 2 (Lecture 2)\n",
    "section": "",
    "text": "3.1.1 R2\n\nR² indicates the proportion of variance in the dependent variable that is explained by the independent variable(s) in the model. An R² value close to 1 indicates that the model explains a large portion of the variance in the outcome variable.\nA problem with the R², is that, it will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. A solution is to adjust the R² by taking into account the number of predictor variables.\nThe adjustment in the “Adjusted R Square” value in the summary output is a correction for the number of x variables included in the prediction model.\n\ndata.frame(\n  model = c(\"model0\", \"model1\", \"model2\", \"model3\", \"model4\"),\n  R2 = c(summary(model0)$r.squared,\n         summary(model1)$r.squared,\n         summary(model2)$r.squared,\n         summary(model3)$r.squared,\n         summary(model4)$r.squared)\n) |&gt; \n  kable(digit = 3)\n\n\n\nmodel\nR2\n\n\n\nmodel0\n0.000\n\n\nmodel1\n0.515\n\n\nmodel2\n0.523\n\n\nmodel3\n0.523\n\n\nmodel4\n0.525\n\n\n\n\n\n\n3.1.2 Model comparison\nWhen comparing models, we can use the anova() function to formally test whether adding predictors improves model fit by reducing unexplained variation. The idea is to fit a reduced model (e.g., with only predictor) and a full model (e.g., with an additional predictor), and then compare them with anova(reduced, full). The output shows how much residual variance is explained by the additional predictors, along with an F-test and p-value. A significant result indicates that the new predictors account for meaningful variation in the outcome, demonstrating that the full model provides a statistically better fit than the simpler one.\nlm(grades ~ 1) vs lm(grade ~ study_time)\n\nanova(model0, model1) |&gt; \n  tidy() %&gt;%\n  mutate(p.value = format(p.value, scientific = FALSE, digits = 4))\n\n# A tibble: 2 × 7\n  term               df.residual   rss    df sumsq statistic p.value            \n  &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;              \n1 grade ~ 1                   99  78.4    NA  NA         NA  \"                 …\n2 grade ~ study_time          98  38.1     1  40.4      104. \"0.000000000000000…\n\n\nlm(grade ~ study_time) vs lm(grade ~ study_time + GPA)\n\nanova(model1, model2) |&gt; \n  tidy() |&gt; \n  mutate(p.value = format(p.value, scientific = FALSE, digits = 4))\n\n# A tibble: 2 × 7\n  term                     df.residual   rss    df  sumsq statistic p.value \n  &lt;chr&gt;                          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n1 grade ~ study_time                98  38.1    NA NA         NA    \"    NA\"\n2 grade ~ study_time + GPA          97  37.4     1  0.632      1.64 \"0.2037\"\n\n\nlm(grade ~ study_time) vs lm(grade ~ study_time + lecture)\n\nanova(model1, model3) |&gt; \n  tidy() |&gt; \n  mutate(p.value = format(p.value, scientific = FALSE, digits = 4))\n\n# A tibble: 2 × 7\n  term                          df.residual   rss    df  sumsq statistic p.value\n  &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 grade ~ study_time                     98  38.1    NA NA        NA     \"    N…\n2 grade ~ study_time + GPA + l…          96  37.4     2  0.656     0.842 \"0.433…\n\n\nlm(grade ~ study_time) + lm(grade ~ study_time + nclicks)\n\nanova(model1, model4) |&gt; \n  tidy() |&gt; \n  mutate(p.value = format(p.value, scientific = FALSE, digits = 4))\n\n# A tibble: 2 × 7\n  term                          df.residual   rss    df  sumsq statistic p.value\n  &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 grade ~ study_time                     98  38.1    NA NA        NA     \"    N…\n2 grade ~ study_time + GPA + l…          95  37.2     3  0.810     0.689 \"0.561…\n\n\nModel 1 is the best because adding more predictors did not significantly decrease the residual variance, meaning the extra predictors did not meaningfully improve the model’s fit.\nIn linear regression, Residual Sum of Squares (RSS) is the sum of the squared differences between the actual and predicted values of the dependent variable. It measures the error remaining in a model and is a key metric for assessing model fit. The goal of ordinary least squares (OLS) linear regression is to minimize the RSS to find the best-fitting regression line that explains the data. A smaller RSS indicates a better-fitting model because it means the predicted values are closer to the actual data points. However, such difference might not be significantly different.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression 2 (Lecture 2)</span>"
    ]
  },
  {
    "objectID": "mr_2.html#centering-and-standardizing-variables",
    "href": "mr_2.html#centering-and-standardizing-variables",
    "title": "\n3  Multiple regression 2 (Lecture 2)\n",
    "section": "\n3.2 Centering and Standardizing variables",
    "text": "3.2 Centering and Standardizing variables\n\n3.2.1 Centering\nWhen fitting a linear regression model:\n\nmodel2 &lt;- lm(grade ~ GPA + nclicks, grades)\ncoef(model2)\n\n(Intercept)         GPA     nclicks \n1.517891154 0.218902581 0.005640213 \n\n\nThe intercept estimate (1.52) represents the predicted grade when GPA is 0. However, in this context, a GPA of 0 is unrealistic, making the intercept’s meaning less useful. Similar issues arise with other predictors, such as age, where a value of 0 may not make practical sense. One way to improve the interpretability of the intercept is to center the variable values—subtracting the mean from each observation—so that the intercept corresponds to the predicted outcome for an average value of the predictor.\nOption 1: create a new variable which contains the centered values of the predictor\n\n# create a new variable and substract the mean from observed data points\ngrades$c_GPA &lt;- grades$GPA - mean(grades$GPA,na.rm = TRUE)\n\n# print first 5 rows\nhead(grades$c_GPA, 5)\n\n[1] -1.2685647 -1.4317918  0.9405614  0.3569027 -1.3804514\n\n\nOption 2: using scale()\n\ngrades &lt;- grades |&gt; \n  mutate(c_GPA = scale(grades$GPA, scale = FALSE))\n\nhead(grades$c_GPA, 5)\n\n           [,1]\n[1,] -1.2685647\n[2,] -1.4317918\n[3,]  0.9405614\n[4,]  0.3569027\n[5,] -1.3804514\n\n\nNote that both options yield the same centered values. With the newly created variable c_GPA, we can refit the model:\n\nlm(grade ~ c_GPA, grades) |&gt; \n  coef()\n\n(Intercept)       c_GPA \n  2.5983884   0.2481337 \n\n\nThe intercept now represents the estimated value of grade when GPAis at its mean (2.5983884), not at 0. Centering does not change the statistical results of the model—such as the slopes, standard errors, or p-values—it only shifts the intercept to a more meaningful reference point.\n\n3.2.2 Standardizing variables\nWhen fitting a model using lm(), the regression coefficients are unstandardized.\n\n# Fit a model\nmodel2 &lt;- lm(grade ~ GPA + nclicks, grades)\n\n# Use coef() to print the coefficients\ncoef(model2)\n\n(Intercept)         GPA     nclicks \n1.517891154 0.218902581 0.005640213 \n\n\nThe regression coefficients for GPA and nclicks are unstandardized effect sizes, meaning they represent the change in the dependent variable in its original units for each one-unit change in the independent variable. Because these variables may be on very different scales, comparing their coefficients directly is difficult. By standardizing the coefficients—expressing them in terms of standard deviations—we can compare the relative strength of predictors within the same model and even across different models. This provides a more directly interpretable measure of effect size.\nLike centering variables, when standardizing (or scaling) variables, we center the variables around a mean. However, when standardizing a variable, we are actually converting the variable to a z-score, which means we set the mean to 0 and the variance to 1; because the standard deviation is just the square root of the variance, then the standard deviation is also set to 1. So how do you interpret a variable that is standardized?\nLet’s assume that we standardized a variable called age for a sample of individuals, where age is measured in years and its mean is 15 years with a standard deviation of 5. This would mean, for example, that a person who is 20 years old has an age that is exactly 1 standard deviation higher than the mean (20 - 5 = 15). If we standardize the age variable, then the mean becomes 0 and the standard deviation becomes 1.0. Accordingly, the standardized score for the person who has an age of 20 years (which was 1 standard deviation above the mean) would become 1.0. If a person has an age of 30, then that means they have a standardized score of 3, which represents 3 standard deviations above the mean (30 - 15 = 15 and 15 / 5 = 3).\nTo standardize the regression estimates—that is, to put all variables on the same scale so their coefficients can be directly compared—we have two main options:\nOption 1: Use scale() from base R before fitting the model\n\nlm(formula = scale(grade) ~ scale(GPA) + scale(nclicks), data = grades)\n\n\nCall:\nlm(formula = scale(grade) ~ scale(GPA) + scale(nclicks), data = grades)\n\nCoefficients:\n   (Intercept)      scale(GPA)  scale(nclicks)  \n     4.365e-17       2.201e-01       9.813e-02  \n\n\nWe could also apply scale() to the data frame:\n\ngrades |&gt; \n  mutate(s_grade = scale(grade),\n         s_GPA = scale(GPA),\n         s_nclicks = scale(nclicks)) |&gt; \n  lm(s_grade ~ s_GPA + s_nclicks, data = _)\n\n\nCall:\nlm(formula = s_grade ~ s_GPA + s_nclicks, data = mutate(grades, \n    s_grade = scale(grade), s_GPA = scale(GPA), s_nclicks = scale(nclicks)))\n\nCoefficients:\n(Intercept)        s_GPA    s_nclicks  \n  4.365e-17    2.201e-01    9.813e-02  \n\n\nOption 2: Use standardise() in the effectsize package after fitting the model\n\nstandardise(model2)\n\n\nCall:\nlm(formula = grade ~ GPA + nclicks, data = data_std)\n\nCoefficients:\n(Intercept)          GPA      nclicks  \n  4.365e-17    2.201e-01    9.813e-02  \n\n\nThe regression coefficients of the predictors are now standardized and can be directly compared to see which has a greater effect on grade.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression 2 (Lecture 2)</span>"
    ]
  },
  {
    "objectID": "mr_3.html",
    "href": "mr_3.html",
    "title": "\n4  Multiple regression 3 (Lecture 3)\n",
    "section": "",
    "text": "4.1 Marginal means and pairwise comparisons\nLoad packages\nLoad data\nFor learning purposes, we will add two new variables: sport, which is categorical, and study_time which is numerical.\nWhen we fit a model in R with a categorical predictor, by default, R uses dummy (treatment) coding, where the reference level is compared to all the other levels of the factor. Each coefficient in the model output represents the difference in the outcome variable between the reference level and one of the other levels.\nHowever, we often want to go beyond these default comparisons. For example, we might want to:\n- Compare specific pairs of levels (not just against the reference);\n- Test whether two levels differ across levels of another factor;\nThese comparisons are not done automatically. To perform them, we can use the emmeans package, which offers functions to:\n- Estimate marginal means;\n- Conduct pairwise comparisons across all levels of a factor;\n- Specify custom comparisons between levels or combinations of levels.\nIn a 3 x 3 factorial design, there are six cell means, six marginal means, one grand mean and 15 possible pairwise comparisons.\ntibble(\n  Percentage = c(\"10\", \"20\", \"30\", \"\"),\n  Soya = c(\"cell mean\", \"cell mean\", \"cell mean\", \"Marginal_means\"),\n  Milk = c(\"cell mean\", \"cell mean\", \"cell mean\", \"Marginal_means\"),\n  Almond = c(\"cell mean\", \"cell mean\", \"cell mean\", \"Marginal_means\"),\n  . = c(\"Marginal_means\", \"Marginal_means\", \"Marginal_means\", \"Grand_mean\"),\n) |&gt; \n  kable(caption = \"3 x 3 Factorial design\", \n        format = \"html\", \n        booktabs = TRUE, \n        escape = TRUE) |&gt; \n  kable_styling() |&gt; \n  row_spec(0, extra_css = \"text-align: center;\") |&gt; \n  column_spec(1:4, extra_css = \"text-align: center;\")\n\n\n3 x 3 Factorial design\n\nPercentage\nSoya\nMilk\nAlmond\n.\n\n\n\n10\ncell mean\ncell mean\ncell mean\nMarginal_means\n\n\n20\ncell mean\ncell mean\ncell mean\nMarginal_means\n\n\n30\ncell mean\ncell mean\ncell mean\nMarginal_means\n\n\n\nMarginal_means\nMarginal_means\nMarginal_means\nGrand_mean\nFor k means, the number of pairwise comparisons is:\n\\[\n{k \\choose 2} \\frac{k(k - 1)}{2}                         \n\\]\nSo with 9 cell means, the number of pairwise comparisons is:\n\\[\n{9 \\choose 2} \\frac{9(9 - 1)}{2} = 36\n\\]\nLet’s simulate data for one dependent variable muscle_mass and two three-level categorical predictors drink and percentage- the latter referring to the percentage of protein in each type of drink.\n# for reproducibility\nset.seed(123)\n\n# Define levels\ndrink &lt;- c(\"soya\", \"milk\", \"almond\")\npercentage &lt;- c(\"10\", \"20\", \"30\")\n\n# Create full 3×3 design with 15 observations per group\ndata &lt;- expand.grid(drink = drink,\n                    percentage = percentage,\n                    rep = 1:15)\n\n# Generate heart rate (hr) with different means per group\ndata$muscle_mass &lt;- with(data,\n  rnorm(\n    n = nrow(data),\n    mean = ifelse(drink == \"milk\", 3, \n                  ifelse(drink == \"almond\", 1, 0)) +\n           ifelse(percentage == \"10\", 0,\n                  ifelse(percentage == \"20\", 1, 3)),\n        sd = 2\n        )\n)\n\nhead(data)\n\n   drink percentage rep muscle_mass\n1   soya         10   1   -1.120951\n2   milk         10   1    2.539645\n3 almond         10   1    4.117417\n4   soya         20   1    1.141017\n5   milk         20   1    4.258575\n6 almond         20   1    5.430130\nLet’s fit the model:\nmodel &lt;- lm(muscle_mass ~ drink + percentage, data)\ntidy(model)\n\n# A tibble: 5 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     0.124     0.345     0.359 7.20e- 1\n2 drinkmilk       2.56      0.378     6.76  4.30e-10\n3 drinkalmond     0.385     0.378     1.02  3.10e- 1\n4 percentage20    1.30      0.378     3.45  7.68e- 4\n5 percentage30    3.39      0.378     8.96  2.94e-15\nLet’s compute the marginal means for the drink variable:\nemmeans(model, specs = \"drink\")\n\n drink  emmean    SE  df lower.CL upper.CL\n soya     1.69 0.268 130     1.16     2.22\n milk     4.25 0.268 130     3.72     4.78\n almond   2.07 0.268 130     1.54     2.60\n\nResults are averaged over the levels of: percentage \nConfidence level used: 0.95\nWe can do the same for percentage:\nemmeans(model, specs = \"percentage\")\n\n percentage emmean    SE  df lower.CL upper.CL\n 10           1.10 0.268 130    0.575     1.63\n 20           2.41 0.268 130    1.879     2.94\n 30           4.50 0.268 130    3.967     5.03\n\nResults are averaged over the levels of: drink \nConfidence level used: 0.95\nWe can also do:\nemmeans(model, specs = \"drink\", by = \"percentage\")\n\npercentage = 10:\n drink  emmean    SE  df lower.CL upper.CL\n soya    0.124 0.345 130   -0.560    0.807\n milk    2.681 0.345 130    1.997    3.364\n almond  0.509 0.345 130   -0.174    1.193\n\npercentage = 20:\n drink  emmean    SE  df lower.CL upper.CL\n soya    1.428 0.345 130    0.744    2.111\n milk    3.985 0.345 130    3.301    4.668\n almond  1.813 0.345 130    1.130    2.497\n\npercentage = 30:\n drink  emmean    SE  df lower.CL upper.CL\n soya    3.516 0.345 130    2.832    4.199\n milk    6.072 0.345 130    5.389    6.756\n almond  3.901 0.345 130    3.217    4.584\n\nConfidence level used: 0.95\nThe second part of the output, called contrasts, contains the comparisons of interest. It is this section that we are generally most interested in when answering a question about differences among groups. You can see which comparison is which via the contrastcolumn.\nBy default, emmeans() calculates all pairwise comparisons.\nem &lt;- emmeans(model, specs = \"drink\")\npairs(em)\n\n contrast      estimate    SE  df t.ratio p.value\n soya - milk     -2.557 0.378 130  -6.756  &lt;.0001\n soya - almond   -0.385 0.378 130  -1.018  0.5668\n milk - almond    2.171 0.378 130   5.738  &lt;.0001\n\nResults are averaged over the levels of: percentage \nP value adjustment: tukey method for comparing a family of 3 estimates\nThe emmeans() package automatically adjusts for multiple comparisons. Since we did all pairwise comparisons the package used a Tukey adjustment. The adjust argument can be used to change the type of multiple comparisons adjustment. All available options are listed and described in the documentation for summary.emmGrid under the section P-value adjustments. For instance, one option is to skip multiple comparisons adjustments all together, using adjust = \"none\" The comparisons are accompanied by statistical tests of the null hypothesis of “no difference”, but lack confidence interval (CI) by default. You can add the 95% CI using confint():\npairs(em) |&gt; \n  confint()\n\n contrast      estimate    SE  df lower.CL upper.CL\n soya - milk     -2.557 0.378 130    -3.45   -1.659\n soya - almond   -0.385 0.378 130    -1.28    0.512\n milk - almond    2.171 0.378 130     1.27    3.069\n\nResults are averaged over the levels of: percentage \nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates\nWe can also compare each level of drink for each level of percentage:\nem &lt;- emmeans(model, specs = \"drink\", by = \"percentage\")\n\n# or, alternatively:\n# emmeans(model, specs = \"drink\", by = \"percentage\")\n\npairs(em)\n\npercentage = 10:\n contrast      estimate    SE  df t.ratio p.value\n soya - milk     -2.557 0.378 130  -6.756  &lt;.0001\n soya - almond   -0.385 0.378 130  -1.018  0.5668\n milk - almond    2.171 0.378 130   5.738  &lt;.0001\n\npercentage = 20:\n contrast      estimate    SE  df t.ratio p.value\n soya - milk     -2.557 0.378 130  -6.756  &lt;.0001\n soya - almond   -0.385 0.378 130  -1.018  0.5668\n milk - almond    2.171 0.378 130   5.738  &lt;.0001\n\npercentage = 30:\n contrast      estimate    SE  df t.ratio p.value\n soya - milk     -2.557 0.378 130  -6.756  &lt;.0001\n soya - almond   -0.385 0.378 130  -1.018  0.5668\n milk - almond    2.171 0.378 130   5.738  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates\nWe can also compute all pairwise comparisons allowed by the study design:\nemm &lt;- emmeans(model, ~ drink * percentage)\n\npairs(emm)\n\n contrast                                  estimate    SE  df t.ratio p.value\n soya percentage10 - milk percentage10      -2.5567 0.378 130  -6.756  &lt;.0001\n soya percentage10 - almond percentage10    -0.3853 0.378 130  -1.018  0.9835\n soya percentage10 - soya percentage20      -1.3040 0.378 130  -3.446  0.0212\n soya percentage10 - milk percentage20      -3.8607 0.535 130  -7.213  &lt;.0001\n soya percentage10 - almond percentage20    -1.6893 0.535 130  -3.156  0.0498\n soya percentage10 - soya percentage30      -3.3916 0.378 130  -8.962  &lt;.0001\n soya percentage10 - milk percentage30      -5.9483 0.535 130 -11.114  &lt;.0001\n soya percentage10 - almond percentage30    -3.7769 0.535 130  -7.057  &lt;.0001\n milk percentage10 - almond percentage10     2.1714 0.378 130   5.738  &lt;.0001\n milk percentage10 - soya percentage20       1.2528 0.535 130   2.341  0.3260\n milk percentage10 - milk percentage20      -1.3040 0.378 130  -3.446  0.0212\n milk percentage10 - almond percentage20     0.8674 0.535 130   1.621  0.7918\n milk percentage10 - soya percentage30      -0.8349 0.535 130  -1.560  0.8244\n milk percentage10 - milk percentage30      -3.3916 0.378 130  -8.962  &lt;.0001\n milk percentage10 - almond percentage30    -1.2202 0.535 130  -2.280  0.3620\n almond percentage10 - soya percentage20    -0.9186 0.535 130  -1.716  0.7354\n almond percentage10 - milk percentage20    -3.4754 0.535 130  -6.493  &lt;.0001\n almond percentage10 - almond percentage20  -1.3040 0.378 130  -3.446  0.0212\n almond percentage10 - soya percentage30    -3.0063 0.535 130  -5.617  &lt;.0001\n almond percentage10 - milk percentage30    -5.5630 0.535 130 -10.394  &lt;.0001\n almond percentage10 - almond percentage30  -3.3916 0.378 130  -8.962  &lt;.0001\n soya percentage20 - milk percentage20      -2.5567 0.378 130  -6.756  &lt;.0001\n soya percentage20 - almond percentage20    -0.3853 0.378 130  -1.018  0.9835\n soya percentage20 - soya percentage30      -2.0876 0.378 130  -5.516  &lt;.0001\n soya percentage20 - milk percentage30      -4.6444 0.535 130  -8.678  &lt;.0001\n soya percentage20 - almond percentage30    -2.4730 0.535 130  -4.621  0.0003\n milk percentage20 - almond percentage20     2.1714 0.378 130   5.738  &lt;.0001\n milk percentage20 - soya percentage30       0.4691 0.535 130   0.876  0.9938\n milk percentage20 - milk percentage30      -2.0876 0.378 130  -5.516  &lt;.0001\n milk percentage20 - almond percentage30     0.0838 0.535 130   0.156  1.0000\n almond percentage20 - soya percentage30    -1.7023 0.535 130  -3.181  0.0465\n almond percentage20 - milk percentage30    -4.2590 0.535 130  -7.958  &lt;.0001\n almond percentage20 - almond percentage30  -2.0876 0.378 130  -5.516  &lt;.0001\n soya percentage30 - milk percentage30      -2.5567 0.378 130  -6.756  &lt;.0001\n soya percentage30 - almond percentage30    -0.3853 0.378 130  -1.018  0.9835\n milk percentage30 - almond percentage30     2.1714 0.378 130   5.738  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 9 estimates\nNote that there are 36 pairwise comparisons which is the same number we previously determined.\nOr only perform comparisons of interest, such as performing comparisons against a reference level. Suppose we are only interested in comparing all types of drinks and percentages against “milk with 20% protein content.\nem &lt;- emmeans(model, ~ drink*percentage)\n\ncontrast(\n  em, \n  method = \"trt.vs.ctrl\", \n  ref = which(emm@grid$drink == \"milk\" & emm@grid$percentage == \"20\")\n  )\n\n contrast                                estimate    SE  df t.ratio p.value\n soya percentage10 - milk percentage20    -3.8607 0.535 130  -7.213  &lt;.0001\n milk percentage10 - milk percentage20    -1.3040 0.378 130  -3.446  0.0056\n almond percentage10 - milk percentage20  -3.4754 0.535 130  -6.493  &lt;.0001\n soya percentage20 - milk percentage20    -2.5567 0.378 130  -6.756  &lt;.0001\n almond percentage20 - milk percentage20  -2.1714 0.378 130  -5.738  &lt;.0001\n soya percentage30 - milk percentage20    -0.4691 0.535 130  -0.876  0.8841\n milk percentage30 - milk percentage20     2.0876 0.378 130   5.516  &lt;.0001\n almond percentage30 - milk percentage20  -0.0838 0.535 130  -0.156  0.9996\n\nP value adjustment: dunnettx method for 8 tests\nSuppose we are only interest in testing whether the different drinks differ in how they affect muscle growth when the protein content is 10%.\nmodel &lt;- lm(muscle_mass ~ drink + as.factor(percentage), data)\n\nemm_10 &lt;- emmeans(model, ~ drink, at = list(percentage = 10))\n\nemm_10 \n\n drink  emmean    SE  df lower.CL upper.CL\n soya    0.124 0.345 130   -0.560    0.807\n milk    2.681 0.345 130    1.997    3.364\n almond  0.509 0.345 130   -0.174    1.193\n\nConfidence level used: 0.95\ncontrast(\n  emm_10, \n  method = \"pairwise\", \n  adjust = NULL) # no multiple comparisons\n\n contrast      estimate    SE  df t.ratio p.value\n soya - milk     -2.557 0.378 130  -6.756  &lt;.0001\n soya - almond   -0.385 0.378 130  -1.018  0.3105\n milk - almond    2.171 0.378 130   5.738  &lt;.0001",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple regression 3 (Lecture 3)</span>"
    ]
  },
  {
    "objectID": "mr_3.html#marginal-means-and-pairwise-comparisons",
    "href": "mr_3.html#marginal-means-and-pairwise-comparisons",
    "title": "\n4  Multiple regression 3 (Lecture 3)\n",
    "section": "",
    "text": "Perform custom contrasts.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple regression 3 (Lecture 3)</span>"
    ]
  },
  {
    "objectID": "mr_3.html#testing-non-zero-effects",
    "href": "mr_3.html#testing-non-zero-effects",
    "title": "\n4  Multiple regression 3 (Lecture 3)\n",
    "section": "\n4.2 Testing non-zero effects",
    "text": "4.2 Testing non-zero effects\nScientists typically test whether the difference between two interventions is greater than, less than, or simply different from zero. However, hypothesis testing is not limited to a null value of zero — we can test whether a difference is greater than (superiority), not worse than (non-inferiority), equivalent to (equivalence), or different from any non-zero value, depending on the scientific question.\nFor example, suppose we are interested in testing the following hypothesis:\nH: the difference in muscle_mass between “soya and almond” will not be greater than -1:\n\ncomparisons &lt;- pairs(emm_10, adjust = NULL) \ncomparisons\n\n contrast      estimate    SE  df t.ratio p.value\n soya - milk     -2.557 0.378 130  -6.756  &lt;.0001\n soya - almond   -0.385 0.378 130  -1.018  0.3105\n milk - almond    2.171 0.378 130   5.738  &lt;.0001\n\n\n\nnon_inferiority &lt;- comparisons[2, ] |&gt; # select first comparison: \"soya and almond\"\n  test(side = \"&gt;\", null = -1)          # test if difference is ≤ -1 (non-inferiority test)\n\nnon_inferiority\n\n contrast      estimate    SE  df null t.ratio p.value\n soya - almond   -0.385 0.378 130   -1   1.624  0.0534\n\nP values are right-tailed \n\n\nThe p-value of 0.0533836 is not smaller than 0.05 so we cannot reject the null hypothesis of inferiority. This conclusion is also supported by the 95% confidence interval, which includes –1 — the non-inferiority margin. Had the lower bound of the confidence interval been greater than –1, we could have rejected the null hypothesis of inferiority and concluded that almond is not worse than soya within the specified margin.\n\nsummary(comparisons[2, ], infer = c(TRUE, TRUE))\n\n contrast      estimate    SE  df lower.CL upper.CL t.ratio p.value\n soya - almond   -0.385 0.378 130    -1.13    0.363  -1.018  0.3105\n\nConfidence level used: 0.95 \n\n\nNow suppose we are interested in testing the following hypothesis:\nH: the difference in muscle_mass between “milk and almond” will be greater than 1 (superiority):\n\nsuperiority &lt;- comparisons[3, ] |&gt; # select first comparison: \"milk and almond\"\n  test(side = \"&gt;\", null = 1)  # test if difference is &gt; 1 (superiority test)\n\nsuperiority\n\n contrast      estimate    SE  df null t.ratio p.value\n milk - almond     2.17 0.378 130    1   3.095  0.0012\n\nP values are right-tailed \n\n\nThe p-value of 0.0012045 is smaller than 0.05 so we can reject the null hypothesis of non-superiority and therefore claim that milk is superior to almond. This conclusion is also supported by the 95% confidence interval, which excludes 1 — the non-superiority margin. Had the upper bound of the confidence interval been smaller than 1, we could have not rejected the null hypothesis of non-superiority.\n\nsummary(comparisons[3, ], infer = c(TRUE, TRUE))\n\n contrast      estimate    SE  df lower.CL upper.CL t.ratio p.value\n milk - almond     2.17 0.378 130     1.42     2.92   5.738  &lt;.0001\n\nConfidence level used: 0.95 \n\n\n\n4.2.1 Predictions\nIn the sections above, the goal was to quantify the influence of several independent variables on a primary outcome. To do this, we used observed data and focused on understanding relationships — for example, how variables like GPA or nclicks relate to students’ final grades. This approach aligns with explanatory modeling, where the emphasis is on interpreting the influence of predictors.\nHowever, if our goal shifts to estimating a student’s grade based on new or unobserved data, then we’re moving into predictive modeling. In this context, we’re less concerned with the underlying relationships and more focused on the accuracy of predictions. In R, we can use predict() on a fitted model to generate grade estimates for new cases. For instance, in the grades data set, the range of nclicks was:\n\nrange(grades$nclicks)\n\n[1]  54 129\n\n\nNone of the students clicked more than 129 times.\nWe can use predict() to estimate the grade for a higher number of clicks than those observed in the original dataset.\n\n# Fit a model\nmodel1 &lt;- lm(grade ~ nclicks, grades)\n\n# Create a new data set with new data\nnew_values &lt;- data.frame(nclicks = c(150, 160, 170))\n\n# Use the fitted model to predict grade based on the new data\npredict(model1, new_values)\n\n       1        2        3 \n3.085685 3.179977 3.274268 \n\n\npredict() returns three estimated grades, one for each new value of nclicks in the new_values data frame.\nWe can also use predict() with multiple predictors, allowing us to estimate a student’s grade using a combination of values.\nBefore doing so, we might want to inspect the range of one of the new predictors. For example, to check the observed range of the lecture variable:\n\nrange(grades$lecture)\n\n[1]  2 10\n\n\nSuppose we now want to predict grades using both nclicks and lecture:\n\n# Fit a model with the predictors of interest\nmodel2 &lt;- lm(grade ~ nclicks + lecture, grades)\n\n# Create a new data set with new data for each predictor\nnew_values &lt;- data.frame(\n  nclicks = c(150, 160, 170),\n  lecture = c(12, 15, 18)\n)\n\npredict(model2, new_values)\n\n       1        2        3 \n3.317920 3.642947 3.967974 \n\n\npredict() again returns three predicted grades, each corresponding to the combination of nclicks and lecture values in the new_values data frame.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple regression 3 (Lecture 3)</span>"
    ]
  },
  {
    "objectID": "mr_3.html#transformations",
    "href": "mr_3.html#transformations",
    "title": "\n4  Multiple regression 3 (Lecture 3)\n",
    "section": "\n4.3 Transformations",
    "text": "4.3 Transformations\n\n4.3.1 Log transformation\nSometimes the data we collect or analyse does not follow a linear pattern. A common example in exercise science is the gain in strength over time. Initially, strength tends to increase rapidly, but these gains gradually slow and approach a plateau.\n\n# for reproducibility\nset.seed(0001)\n\n# set parameters\nb0 &lt;- 60\nb1 &lt;- 4\nyears &lt;- seq(1:36)\n\n# simulate strength gains over a 36-month period \ndata &lt;- data.frame(\n  x = years,\n  y = b0 + b1 * log(years) + rnorm(length(years), 0, 0.5)\n)\n\n# plot data\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, colour = \"red\") +\n  labs(y = \"strength gain\", x = \"months\")\n\n\n\n\n\n\n\nIf we fit a simple linear regression to this type of data, the straight line will fail to capture this curved pattern, resulting in a poor fit compared to a model that accounts for the non-linear relationship.\n\nsummary(lm(y ~ x, data))\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8099 -0.6237  0.1164  1.1264  1.8963 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.20124    0.53161  122.65  &lt; 2e-16 ***\nx            0.29545    0.02506   11.79 1.46e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.562 on 34 degrees of freedom\nMultiple R-squared:  0.8035,    Adjusted R-squared:  0.7977 \nF-statistic:   139 on 1 and 34 DF,  p-value: 1.459e-13\n\n\nWhen we fit a model that accounts for the non-linear relationship—such as applying a log transformation to capture the rapid early gains and later plateau—the resulting curve provides a far better representation of the observed strength gains than a simple linear model.\n\n# plot data\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ log(x), se = FALSE) +\n  labs(y = \"strength gain\", x = \"months\")\n\n\n\n\n\n\n\nFitting a model that better captures the true shape of the data generally results in smaller residuals improving the goodness of fit metrics like the Residual Standard Error (RSE) and R2. Briefly, RSE is the square root of the mean square error and it shows It shows how much the observed values deviate from the predicted values. The lower the value the better a regression model fits data.\n\nlinear_model &lt;- lm(y ~ x, data)\nlog_model &lt;- lm(y ~ log(x), data)\n\n\ntibble::tibble(\n  model = c(\"linear model\", \"log model\"),\n  RSE = c(\n    sqrt(sum(resid(linear_model)^2) / linear_model$df.residual),\n    sqrt(sum(resid(log_model)^2) / log_model$df.residual)),\n  R_squared = c(\n    summary(linear_model)$r.squared,\n    summary(log_model)$r.squared)\n  ) |&gt; \n  kable(format = \"html\", \n        booktabs = TRUE, \n        escape = TRUE) |&gt; \n  kable_styling()\n\n\n\nmodel\nRSE\nR_squared\n\n\n\nlinear model\n1.5617240\n0.8035176\n\n\nlog model\n0.4606295\n0.9829070\n\n\n\n\n\n\n4.3.2 Tukkey’s ladder of powers\nThe Tukkey ladder of powers (sometimes called the Bulging Rule) is a way to change the shape of a skewed distribution so that it becomes normal or nearly-normal. It can also help to reduce error variability (heteroscedasticity).\nTukkey (1977) created a table of powers (numbers to which data can be raised). It’s possible to have an infinite number of powers, but very few are actually in common use. Table 4.1 shows the most commonly used transformations, with exponents ranging from -2 to 2.\n\n\n\nTable 4.1: Types of power transformations\n\n\n\n\nPower\nTransformation\nName\n\n\n\n2.0\nY^2\nSquare\n\n\n1.0\nY\nIdentity\n\n\n0.5\n1/sqrt(Y)\nReciprocal sqrt\n\n\n0.0\nlog(Y)\nLogarithm\n\n\n-0.5\n-1/sqrt(Y)\nReciprocal root\n\n\n-1.0\n1/Y\nReciprocal\n\n\n-2.0\n1/Y^2\nReciprocal square\n\n\n\n\n\n\n\n\nExample\nLet’s simulate right-skewed data:\n\n# for reproducibility\nset.seed(123)\n\n# simulate right-skewed data\ny &lt;- rexp(200, rate = 0.5)  \n\n# plot data to see distribution\nhist(y, breaks = 20)\n\n\n\n\n\n\n\nThis histogram shows that the data is not normally distributed, which we can also confirm using shapiro.test():\n\n# asses normality of data\nshapiro.test(y) |&gt; \n  tidy() |&gt; \n  mutate(pvalue = format(p.value, scientific = FALSE)) |&gt; \n  select(-p.value)\n\n# A tibble: 1 × 3\n  statistic method                      pvalue                 \n      &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt;                  \n1     0.807 Shapiro-Wilk normality test 0.000000000000005466922\n\n\nSince the data is skewed, we can apply Tukey’s power transformations using transformTukey() to try to normalize it. transformTukey() simply loops through lamdba values and chooses the lambda that results in best normality. For example:\n\ntransformTukey(y, plotit = TRUE)\n\n\n\n\n\n\n\n\n    lambda      W Shapiro.p.value\n414  0.325 0.9921          0.3555\n\nif (lambda &gt;  0){TRANS = x ^ lambda} \nif (lambda == 0){TRANS = log(x)} \nif (lambda &lt;  0){TRANS = -1 * x ^ lambda} \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  [1] 1.1852371 1.0474216 1.3739983 0.4075040 0.4915035 0.8618997 0.8598823\n  [8] 0.6691736 1.7353732 0.3970626 1.2546276 0.9869639 0.8292226 0.9124095\n [15] 0.7280298 1.1881202 1.4484043 0.9859915 1.0558084 1.9721686 1.1850967\n [22] 1.2386069 1.4245315 1.3803481 1.3177031 1.4611307 1.4280966 1.4506438\n [29] 0.4083009 1.0598079 1.6108073 1.0042812 0.8080921 1.7081746 1.3394980\n [36] 1.1606073 1.0776036 1.3485082 1.0545001 1.3031564 0.9451774 2.3805836\n [43] 1.1862705 0.7720291 1.2922033 1.6300005 1.3855491 1.0472925 1.7351745\n [50] 1.3682983 0.5739683 0.8526842 1.2794295 0.8592495 1.2422504 1.5400052\n [57] 1.0402739 1.7039028 1.2717776 1.2625339 1.2639055 0.8327124 1.4483586\n [64] 0.4473909 0.5900502 0.5899304 0.8286195 0.8431463 1.2413340 1.2209062\n [71] 1.4718576 1.4652763 1.6950845 1.4357464 0.9146811 0.7861875 0.9777048\n [78] 0.4480228 0.8647009 1.0855188 1.0450266 0.7613222 2.0421479 1.5318121\n [85] 1.1079717 1.4100967 1.4972484 1.3450555 1.4176471 1.4405885 0.2178734\n [92] 1.2954112 0.8470035 1.3262484 1.2977471 0.5213132 0.9872671 1.4505843\n [99] 0.8084847 1.5317670 0.9754736 0.7835244 1.3226572 0.5011396 0.9324861\n[106] 1.2289722 0.9424036 1.1424417 0.7285350 1.2002897 0.7302268 1.2440860\n[113] 0.8679400 1.3711101 0.8636303 1.4608985 0.6698725 1.5172023 0.4010316\n[120] 1.3653355 0.7422032 1.5029326 1.5063304 1.1850482 0.8895144 1.8461148\n[127] 0.9313330 1.2923418 1.3739540 1.0845816 0.7373916 0.9710516 0.9089616\n[134] 1.8757763 1.3552452 1.2849654 0.8442308 0.5603964 1.7811050 1.5610291\n[141] 1.0964998 1.4621767 1.0649597 0.5802007 0.8690372 1.5052787 0.8146668\n[148] 1.2279913 0.9657150 1.3603689 0.7152375 1.4509660 0.5077877 1.0438509\n[155] 1.4971435 1.3678172 1.3624124 0.9782652 0.4129330 0.6806041 1.2681279\n[162] 1.2853362 1.9243892 0.2326226 0.2142428 1.4884331 1.0257399 0.9803411\n[169] 0.4778976 1.0110440 1.4238664 1.3571294 1.4059502 1.5987490 1.4052105\n[176] 1.2035108 1.7280488 1.6256534 1.0146004 0.5244983 1.0429982 2.0223629\n[183] 1.0363923 0.9724261 0.8088056 1.1149564 1.2374877 1.1316196 1.2967441\n[190] 1.0642677 1.1005440 1.4687209 0.5963862 0.9360387 1.7382876 1.7609713\n[197] 0.3736107 0.9838679 1.0987782 1.2320523\n\n\nThe argument plotit = TRUE produces plots of: (1) Shapiro-Wilks W vs. lambda, (2) histogram of transformed values and Q-Q plot of transformed values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple regression 3 (Lecture 3)</span>"
    ]
  },
  {
    "objectID": "mr_4.html",
    "href": "mr_4.html",
    "title": "\n5  Non-linear Regression\n",
    "section": "",
    "text": "5.1 Non-Linear Regression\nRequired packages\nLinear regression does not work for:\nGeneralized linear models, which extend the linear regression model to work with binary, discrete and count outcomes and other data that cannot be fit well with normally distributed errors.\npar(mfrow = c(2, 2))\nhist1 &lt;- hist(rnorm(4000, 0, 1), main = \"Normal distirbution\", xlab = NULL)\nhist2 &lt;- hist(rbinom(4000, 1, 0.05), main = \"Binomial distirbution\", xlab = NULL)\nhist3 &lt;- hist(rgamma(4000, 1, 100), main = \"Gamma distirbution\", xlab = NULL)\nhist4 &lt;- hist(rpois(4000, 0.5), main = \"Poisson distirbution\", xlab = NULL)\nTypes of distirbutions\n\nDistribution\nDescription\n\n\n\nNormal\nStandard linear regression is useful when data follows a bell-shaped distribution.\n\n\nBinomial\nBinary logisitic regression.Useful when the response is binary (e.g., yes/no).\n\n\nGamma\nGamma regression is useful when data is highly positively skewed.\n\n\nPoisson\nPoisson regression is useful for count data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  },
  {
    "objectID": "mr_4.html#non-linear-regression",
    "href": "mr_4.html#non-linear-regression",
    "title": "\n5  Non-linear Regression\n",
    "section": "",
    "text": "binary outcomes: For example, predicting a disease (yes/no) based on the age, sex, smoking and blood pressure. Alternative: logistic regression\ndiscrete outcomes: For example, classify severity as low, medium or high. Alternative: ordinal logistic regression.\ncount outcomes: For example, number of hospital visits, number of cigarettes smoked per day. Alternative: poisson regression or negative binomial regression\n\n\n\n\n\n5.1.1 Logistic regression\nIn linear regression, the outcome is a continuous variable, meaning that it can take on a range of numerical values. In contrast, logistic regression is used when the outcome is binary-that is, a categorical variable with exactly two possible values, such as “yes” or “nor”, “success” or “failure”, or “disease” or “no disease”. To model binary data, we need to add two features to the base model y = b0+ b1x1:\n\na nonlinear transformation that bounds the output between 0 and 1 (unlike y = b0+ b1x1, which is continuous), and\na model that treats the resulting numbers as probabilities and maps them into random binary outcomes.\n\nLogistic regression models the probability of one of these outcomes occurring as a function of one or more predictor variables. For example, logistic regression can be used to model the relationship between between heart coronary disease (yes or no) and predictors such as age and blood pressure. Similarly, it can be used to model the probability of a student dropping out of school (yes or no) based socioeconomic factors.\nThe model of a logistic regression\nIn logistic regression, the model estimates the log-odds of the binary outcome y as a linear function of predictors:\n\\[\nlog(\\frac{P(Y = 1)}{P(Y = 0)}) = b_0 + b_{1}x_{1} + b_{2}x_{2} + ... + b_{p}x_{p}\n\\]\n\nb0: the intercept is log-odds of the outcome when all predictors are equal to zero. Exponentiating b0 gives the odds of the outcome when all predictors are equal to 0.\nIf x is a continuous predictor the regression coefficient b represents the change in the log-odds of the outcome per one-unit increase in x, holding all other predictors constant. Equivalently, exp(b) is the odds ratio (OR) for a one-unit increase in x.\nIf x is a categorical predictor (with k levels) is represented using dummy variables, comparing each level to a reference category. The coefficient b for a given level represents the difference in log-odds of the outcome between that category and the reference category, again holding other variables constant. Equivalently, exp(b) is the odds ratio comparing that level to the reference level.\n\n\n5.1.1.1 Why not to use a linear regression when outcome is binary\nFirst let’s simulate data where disease is a binary outcome and cholesterol is a continuous predictor.\n\n# for reproducibility\nset.seed(050990)\n\n# create a binomial variable\ndisease &lt;- rbinom(500, 1, 0.5)\n\n# create a continuous predictor\ncholesterol &lt;- ifelse(disease == 1, rnorm(500, 220, 10), rnorm(500, 190, 10))\n\n# create a data set\ndisease_df &lt;- data.frame(disease = disease, cholesterol = cholesterol)\n\nhead(disease_df, 5)\n\n  disease cholesterol\n1       1    207.6714\n2       1    254.9519\n3       1    235.0051\n4       1    193.6956\n5       0    193.1898\n\n\nNow let’s illustrate the relationship between disease and cholesterol.\n\nggplot(disease_df, aes(y = disease, x = cholesterol)) +\n  geom_point() +\n  ylim(c(0, 1)) +\n  xlim(c(140, 300)) +\n  geom_smooth(method = lm, se = FALSE, colour = \"red\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThe straight line fitted using a linear model poorly fits the data. However, the red S-shaped curve better reflects the probability structure of binary data.\n\nggplot(disease_df, aes(y = disease, x = cholesterol)) +\n  geom_point() +\n  ylim(c(0, 1)) +\n  xlim(c(140, 300)) +\n  geom_smooth(method = glm, method.args= list(family = \"binomial\"), se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAnother key problem when using a linear regression model for binary outcomes is that the estimated values (interpreted as probabilities) can fall outside the range 0-1.\n\nlinear_model &lt;- lm(disease ~ cholesterol, disease_df)\n\ntidy(linear_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -4.10    0.136        -30.0 6.28e-114\n2 cholesterol   0.0225  0.000667      33.8 4.79e-131\n\n\nFor example, when cholesterol is 0, the linear model estimates that the probability of disease is -4.1 - which is not a valid probability. In contrast, logistic regression constrains the estimated values within the range [0, 1] making it appropriate for classification tasks. In R we can fit a logistic linear regression using glm(family = binomial).\n\nNote\nContrast with Linear Regression:\n\nLinear regression minimizes the vertical distance (squared errors) between observed data points and the fitted line.\nLogistic regression maximizes the likelihood - the joint probability of the observed outcome given the model’s predicted probabilities (i.e., the parameter estimates are those values which maximize the likelihood of the data which have been observed).\n\n\n\n5.1.1.2 Interpreting logistic regression coefficients\nTo interpret the logistic regression output, we need to transform the coefficients from the log-odds scale to OR and probabilities.\n1- Fit a logistic simple model:\n\n# fit a logistic simple model\nlogistic_model1 &lt;- glm(disease ~ cholesterol, family = binomial, data = disease_df)\nlogistic_model1$coefficients\n\n(Intercept) cholesterol \n-60.2681245   0.2956278 \n\n\nAs it appears now, (Intercept) and cholesterol represent the log-odds of disease.\n2- Convert coefficients to Odds Ratios using exp():\n\nor_b1 &lt;- exp(coef(logistic_model1)[[2]])\nor_b1\n\n[1] 1.34397\n\n\nEach 1-unit increase in cholesterol is associated with 1.34x higher odds of having the disease.\n3- Estimate the probability of disease for a given cholesterol level\n\nintercept &lt;- coef(logistic_model1)[[1]]\nslope &lt;- coef(logistic_model1)[[2]]\n\n# Cholesterol value\ncholesterol &lt;- 200\n\n# Compute log-odds\nlog_odds &lt;- intercept + slope*cholesterol\n\n#Convert to probability of disease\nprob &lt;- 1 / (1 + exp(-log_odds))\nprob\n\n[1] 0.2418507\n\n\nAlternatively, we can use predict(type = response) to calculate the probability of disease`\n\n# Create a data frame with new values\nnew_data &lt;- data.frame(cholesterol = c(200, 250))\n\n# Use predict(type = \"response\") to estimate probabilities for any given value \npredict(logistic_model1, new_data, type = \"response\")\n\n        1         2 \n0.2418507 0.9999988 \n\n\n\n5.1.1.3 About odds and odds ratios\n\n5.1.1.3.1 Odds\nThe odds of an event are the ratio of how likely the event is to occur and how likely it is to not occur. Odds express likelihood relative to non-occurrence. Odds are calculated as:\n\\[\n\\frac {p}{(1-p)}\n\\]where p is the probability of an event happening, and 1-p is the probability that the event does not happen. For example, if an event has a probability of 0.8, then the odds are:\n\\[\n\\frac {0.8}{(1-0.8)} = \\frac {0.8}{0.2} = 4\n\\]\nwhich is read as “4 to 1”.\nWhen the probability is greater than 50% (0.5), the odds are always greater than 1. When the probability of an event are is smaller than 50%, the odds are smaller than 1. When the probability is 50%, the odds equal 1.\n\n5.1.1.3.2 Odds ratios\nAn odds ratio (OR) compares the odds of an event occurring in one group to the odds of the same event occuring in another group. It quantifies how much more (or less) likely the event is in one group compared to the other. For example, suppose the probability of disease is 0.25 for patients with high cholesterol and 0.15 for patients with adequate levels. The OR is, therefore, calculated as:\n\nor &lt;- (0.25 / (1-0.25)) / (0.15 / (1-0.15))\nor\n\n[1] 1.888889\n\n\nAn OR of 1.89 implies that the first group has 89% greater odds of developing the disease than the second group.\n\n5.1.1.3.3 An example using a frequency table\n\n\nExposure Status\nDisease = Yes\nDisease = No\nTotal\n\n\n\nSmoker\n30\n70\n100\n\n\nNon-smoker\n10\n90\n100\n\n\n\n\n\nOdds of disease among exposed:\n\\[\n\\frac {30}{70} = 0.43\n\\]\n\n\nOdds of disease among unexposed:\n\\[\n\\frac {10}{90} = 0.11\n\\]\n\n\nOdds Ratio:\n\\[\n\\frac {0.43} {0.11} = 3.86\n\\]\n\n\nInterpretation:\n\nThe odds of disease in the exposed group are 0.43\nThe odds of disease in the unexposed group are 0.11\nThe odd ratio is 3.86, meaning the exposed group has 3.86 times higher odds of developing the disease compared to the unexposed group.\n\n5.1.1.4 Multiple logistic regression\nJust like in the linear regression, we can fit more than one predictor.\n\n5.1.1.4.1 Model with one continuous predictor and one categorical predictor\nLet’s create a second predictor of disease (i.e., smoking):\n\n# for reproducibility\nset.seed(050990)\n\n# create a binary predictor (smoking)\nsmoking &lt;- ifelse(disease == 1, rbinom(500, 1, 0.7), rbinom(500, 1, 0.10))\n\n# add the new variable to the disease_df data set\ndisease_df$smoking &lt;- as.factor(smoking)\n\nhead(disease_df, 5)\n\n  disease cholesterol smoking\n1       1    207.6714       0\n2       1    254.9519       0\n3       1    235.0051       1\n4       1    193.6956       0\n5       0    193.1898       1\n\n\nLet’s fit model with both cholesterol and smoking as predictors of disease.\n\nlogistic_model2 &lt;- glm(disease ~ cholesterol + smoking, family = binomial, disease_df)\n\ntidy(logistic_model2)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -59.9      6.44       -9.30 1.44e-20\n2 cholesterol    0.292    0.0315      9.27 1.87e-20\n3 smoking1       1.86     0.551       3.37 7.55e- 4\n\n\nRather than using the exp() function to convert log-odds coefficient of each single predictor to an OR like this:\n\nexp(coef(logistic_model2)[[2]])\n\n[1] 1.33875\n\n\nWe can use tidy() in the broom package with the argument exponentitate = TRUE to obtain the ORs for all predictors at once\n\nres &lt;- tidy(logistic_model2, exponentiate = TRUE)\nres\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 9.93e-27    6.44       -9.30 1.44e-20\n2 cholesterol 1.34e+ 0    0.0315      9.27 1.87e-20\n3 smoking1    6.41e+ 0    0.551       3.37 7.55e- 4\n\n\nWe can interpret these coefficients as follows:\n\ncholesterol: For each 1-unit increase, the odds of disease increase by 34%. This increase is statistically significant.\nsmoking1: being a smoker (smoking1 = 1) increases the odds of disease by 541 % compared to a non-smoker. This increase is statistically significant.\n\nWe can also obtain the confidence intervals (CIs) for the odds ratios by exponentiating the confidence intervals of the model coefficients:\n\n# subset rows 2 and 3 since these correspond to the predictors\nexp(confint(logistic_model2))[2:3,]\n\nWaiting for profiling to be done...\n\n\n               2.5 %    97.5 %\ncholesterol 1.266617  1.434272\nsmoking1    2.274929 20.022067\n\n\nAlternatively, we can use tidy() in the broom package with the argument conf.int = TRUE (along with exponentiate = TRUE) to get the odds ratios and their confidence intervals all at once:\n\ntidy(logistic_model2, \n     exponentiate = TRUE,\n     conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 9.93e-27    6.44       -9.30 1.44e-20 7.43e-33  8.29e-22\n2 cholesterol 1.34e+ 0    0.0315      9.27 1.87e-20 1.27e+ 0  1.43e+ 0\n3 smoking1    6.41e+ 0    0.551       3.37 7.55e- 4 2.27e+ 0  2.00e+ 1\n\n\nNow let’s simulate the probability of disease:\n\n# Get the levels from the original data\nlevels(disease_df$smoking)\n\n[1] \"0\" \"1\"\n\n# Create new values \nnew_data &lt;- data.frame(\n  cholesterol = c(50, 50, 200, 200),\n  smoking = factor(rep(c(0,1), 2), levels = levels(disease_df$smoking))\n)\n\n# Add predictions\npredicted_prob &lt;- predict(logistic_model2, new_data, type = \"response\")\n\n# Store both numeric and pretty-formatted percentages\nnew_data$probability &lt;- round(predicted_prob * 100, 2)  \n\nnew_data\n\n  cholesterol smoking probability\n1          50       0        0.00\n2          50       1        0.00\n3         200       0       17.84\n4         200       1       58.19\n\n\n\n5.1.1.4.2 Model with an interaction between two predictors\nIn this section, we analyze a dataset containing health records of patients to predict a binary outcome: whether a patient suffers from diabetes (diabetes: yes = 1 / no = 0).\nThe goal is to explore how various physiological indicators—such as body mass index, blood pressure, and glucose levels—are associated with the likelihood of diabetes.\nLoad and clean the data:\n\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\")\n\ncolnames(data)&lt;-tolower(colnames(data))\n\ndata$outcome&lt;-as.factor(data$outcome) \n\nhead(data, 5)\n\n  pregnancies glucose bloodpressure skinthickness insulin  bmi\n1           6     148            72            35       0 33.6\n2           1      85            66            29       0 26.6\n3           8     183            64             0       0 23.3\n4           1      89            66            23      94 28.1\n5           0     137            40            35     168 43.1\n  diabetespedigreefunction age outcome\n1                    0.627  50       1\n2                    0.351  31       0\n3                    0.672  32       1\n4                    0.167  21       0\n5                    2.288  33       1\n\n\nFit a regression model with an interaction term between\n\n# Fit a multiple regression logistic model\nlogistic_model3 &lt;- glm(outcome ~ age + glucose*bmi, data = data, family=\"binomial\")\n\n# Use tidy() to make it more readable\nres &lt;- tidy(logistic_model3, exponentiate = TRUE)\n\n# Make estimates and p-values more interpretable\nres &lt;- res |&gt; \n  mutate(estimate_round = round(estimate, 2),\n         p_value = round(p.value, 2)) |&gt; \n  select(term, estimate_round, std.error, statistic, p_value)\n\nres\n\n# A tibble: 5 × 5\n  term        estimate_round std.error statistic p_value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)           0     2.25        -4.50     0   \n2 age                   1.03  0.00770      3.81     0   \n3 glucose               1.05  0.0177       2.64     0.01\n4 bmi                   1.14  0.0653       2.04     0.04\n5 glucose:bmi           1     0.000508    -0.815    0.42\n\n\nInterpretation:\n\nage: Every 1-year increase in age increases the odds of diabetes by 3%. This increase is statistically significant.\nglucose: Every 1-unit increase in glucose increases the odds of diabetes by 5%. This increase is statistically significant.\nbmi: Every 1-unit increase in body mass index increases the odds of diabetes by 14%. This increase is statistically significant.\nglucos:bmi: The effect of glucose on the odds of diabetes does not increase as bmi increases. The interaction effect is not statistically significant.\n\n5.1.1.5 Centering\n\nglm(outcome ~ glucose + bloodpressure, data = data, family=\"binomial\") |&gt; \ntidy(exponentiate = TRUE)\n\n# A tibble: 3 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.00518   0.486     -10.8   2.57e-27\n2 glucose        1.04      0.00329    11.6   7.22e-31\n3 bloodpressure  0.998     0.00443    -0.354 7.23e- 1\n\n\n(intercept) represents the odds of diabetes when all other variables are 0. However, interpreting (intercept) when glucose = 0 and bloodpressure = 0 is not meaningful as it is physiologically impossible. We can make (intercept) more interpretable by centering the predictors.\n\n# Center the predictors\ndata$glucose_centered &lt;- data$glucose - mean(data$glucose)\ndata$bloodpressure_centered &lt;- data$bloodpressure - mean(data$bloodpressure)\n\n# Refit the logistic regression model with the centered predictors\nres &lt;- glm(outcome ~ glucose_centered + bloodpressure_centered, \n           data = data, family=\"binomial\") |&gt; \n  tidy(exponentiate = TRUE)\n\nres\n\n# A tibble: 3 × 5\n  term                   estimate std.error statistic  p.value\n  &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               0.462   0.0883     -8.74  2.40e-18\n2 glucose_centered          1.04    0.00329    11.6   7.22e-31\n3 bloodpressure_centered    0.998   0.00443    -0.354 7.23e- 1\n\n\nInterpretation:\n\n(intercept): When glucose and bloodpressure are at their average levels, the odds of diabtes are 0.46.\nglucose_centered: Every 1-unit increase in glucose significantly increases the odds of diabetes by 4%.\nbloodpressure: Every 1-unit increase in blood pressure decreases the odds of diabetes by -0.16%. However, this is not statistically significant.\n\n5.1.1.6 Model fit\nSuppose we want to compare whether adding bmi to a simpler model increases the fit of the model:\nSimpler model\n\nmodel1 &lt;- glm(outcome ~ glucose + bloodpressure + insulin, \n             data = data, family=\"binomial\") \n\nsummary(model1)\n\n\nCall:\nglm(formula = outcome ~ glucose + bloodpressure + insulin, family = \"binomial\", \n    data = data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -5.3381664  0.4943004 -10.799   &lt;2e-16 ***\nglucose        0.0390197  0.0034669  11.255   &lt;2e-16 ***\nbloodpressure -0.0013534  0.0044319  -0.305    0.760    \ninsulin       -0.0007271  0.0007565  -0.961    0.336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 807.68  on 764  degrees of freedom\nAIC: 815.68\n\nNumber of Fisher Scoring iterations: 4\n\n\nModel with bmi\n\nmodel2 &lt;- glm(outcome ~ glucose + bloodpressure + insulin + bmi, \n             data = data, family=\"binomial\")\n\nsummary(model2)\n\n\nCall:\nglm(formula = outcome ~ glucose + bloodpressure + insulin + bmi, \n    family = \"binomial\", data = data)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -7.433886   0.657526 -11.306  &lt; 2e-16 ***\nglucose        0.037708   0.003531  10.679  &lt; 2e-16 ***\nbloodpressure -0.007430   0.004907  -1.514   0.1300    \ninsulin       -0.001493   0.000785  -1.902   0.0572 .  \nbmi            0.083887   0.013880   6.044  1.5e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 765.48  on 763  degrees of freedom\nAIC: 775.48\n\nNumber of Fisher Scoring iterations: 4\n\n\nHow do we choose the best model?\n\n5.1.1.6.1 AIC\nAIC (Akaike Information Criterion) is a metric used to compare and select the best model from a set of candidate models by balancing goodness-of-fit and complexity. It is calculated as follows:\n\\[\nAIC = -2 * log(likelihood) + 2k\n\\]\nIn the AIC formula, k represents the number of parameters included in the model-including the intercept and all independent variables. The term 2k serves as a penalty for model complexity. A lower AIC value indicates a model that achieves a better balance between fit and simplicity (parsimony). Therefore, among competing models, the one with the lowest AIC is generally preferred.\nmodel1 had an AIC of 815.6764735 whereas model2` had an AIC of 775.4809783. Since model2 has the lower AIC, it is the preferred model, as it achieves a better trade-off between model fit and complexity.\n\n5.1.1.6.2 Pseudo R2\n\nNote that now glm() does not provide R2 compared with lm(). This is because we need to use another fit estimate when using glm(). Even though everyone agrees on how to calculate R2 and associated p-value for linear Models, there is no consensus on how to calculate R2 for logistic regression. There are more than 10 different ways to do it!\nOne of the most common method is McFadden’s Pseudo R2. This method is very similar to the method in Linear Model. McFadden’s R squared measure is defined as:\n\\[\nR^2_{McFadden} = \\frac {1−log(Lc)} {log(Lnull)}\n\\]\nwhere Lc denotes the (maximized) likelihood value from the current fitted model, and Lnull denotes the corresponding value but for the null model - the model with only an intercept and no covariates.\nThe log-likelihood R2 values go from 0, for poor models, to 1, for good models and it can be calculated with the function pscl::pR2()\nThe log-likelihood R2 for model would be:\n\npR2(model1)\n\nfitting null model for pseudo-r2\n\n\n         llh      llhNull           G2     McFadden         r2ML         r2CU \n-403.8382368 -496.7419551  185.8074366    0.1870261    0.2148942    0.2961125 \n\n\nThe log-likelihood R2 for model2 would be:\n\npR2(model2)\n\nfitting null model for pseudo-r2\n\n\n         llh      llhNull           G2     McFadden         r2ML         r2CU \n-382.7404891 -496.7419551  228.0029319    0.2294984    0.2568659    0.3539473 \n\n\nBecause model2 has a higher McFadden’s R2 value, we conclude that model2 fits the data better than model1.\n\n5.1.1.6.3 Summary\nKey insights:\n\nAIC looks for the best trade-off between fit and complexity.\nPseudo-R² only looks at improvement in fit (ignoring complexity)\n\nSo, a model might:\n\nHave a higher pseudo-R²\nBut a higher (worse) AIC if it’s too complex\n\nFor example:\n\n\n\n\n\n\n\n\n\nModel\nLog-Likelihood\nParameters\nAIC\nMcFadden’s pseudo R²\n\n\n\nNull Model\n-150\n1\n302\n0.00\n\n\nModel A\n-100\n3\n206\n0.33\n\n\nModel B\n-98\n6\n208\n0.35\n\n\n\nDespite Model B having a slightly higher McFadden’s pseudo R2 compared to model A, its AIC value is higher. Since AIC penalizes complexity and is more robust criterion for model selection, we prefer Model A over Model B due to its lower AIC.\n\n5.1.1.7 Model comparison\nTo statistically compare models, we can use anova(test = \"Chisq\").\n\nanova(model1, model2, test = \"Chisq\") |&gt; tidy()\n\n# A tibble: 2 × 6\n  term                    df.residual residual.deviance    df deviance   p.value\n  &lt;chr&gt;                         &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 outcome ~ glucose + bl…         764              808.    NA     NA   NA       \n2 outcome ~ glucose + bl…         763              765.     1     42.2  8.26e-11\n\n\nBased on the Chi-square test from the likelihood ratio test, we conclude that model2 fits the data significantly better than model1 as the p-value is below 0.05.\n\n5.1.1.8 Differences between AIC and anova(test = “Chisq”)\n\n\n\n\n\n\n\nFeature\nAIC\nANOVA (test = \"Chisq\")\n\n\n\nPurpose\nModel selection (fit vs. complexity)\nHypothesis testing (is extra predictor significant?)\n\n\nCompares\nAny models (nested or not)\nOnly nested models (i.e., one is a subset of the other)\n\n\nPenalty for complexity\nYes (via AIC formula)\nNo — tests if added complexity improves fit significantly\n\n\nBased on\nLog-likelihood\nLikelihood ratio test (LRT)\n\n\nOutput\nAIC values\n\np-values\n\n\n\n5.1.2 Poisson regression\nIn this section, we will briefly cover Poisson regression, a modelling technique used to analyze count data. To illustrate this, we will simulate a dataset where the outcome is the number of calls made by customers. We aim to predict calls using two predictors: age (age of customers) and is_premium (type of subscription: 1 = premium; 0 = regular customer).\nLet’s simulate the dataset.\n\n# Simulate a simple dataset\nset.seed(123)\n\nn &lt;- 100  # number of observations\n\ndata &lt;- data.frame(\n  age = sample(20:70, n, replace = TRUE),\n  is_premium = sample(0:1, n, replace = TRUE)\n)\n\n# Simulate expected call rate (log link)\ndata$lambda &lt;- exp(0.5 + 0.03 * data$age - 0.6 * data$is_premium)\n\n# Simulate call counts using Poisson distribution\ndata$calls &lt;- rpois(n, lambda = data$lambda)\n\n# View the first few rows\nhead(data)\n\n  age is_premium    lambda calls\n1  50          1  4.055200     8\n2  34          0  4.572225     6\n3  70          0 13.463738    15\n4  33          0  4.437096     4\n5  22          1  1.750673     2\n6  61          1  5.640654     6\n\n\nNow let’s fit the poisson regression model using glm(family = poisson()).\n\nmodel &lt;- glm(calls ~ age + is_premium, data = data, family = poisson())\n\nsummary(model)\n\n\nCall:\nglm(formula = calls ~ age + is_premium, family = poisson(), data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.43520    0.19898   2.187   0.0287 *  \nage          0.02984    0.00352   8.476  &lt; 2e-16 ***\nis_premium  -0.53497    0.09289  -5.759 8.44e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 248.63  on 99  degrees of freedom\nResidual deviance: 107.94  on 97  degrees of freedom\nAIC: 434.62\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe coefficients are on the log scale of the rate and we use exp() to exponentiate coefficients to get Incidence Rate Ratios (IRRs):\n\nexp(coef(model))\n\n(Intercept)         age  is_premium \n  1.5452686   1.0302855   0.5856854 \n\n\nObtain 95% CI:\n\nexp(confint(model))\n\nWaiting for profiling to be done...\n\n\n               2.5 %    97.5 %\n(Intercept) 1.040433 2.2700495\nage         1.023243 1.0374640\nis_premium  0.487611 0.7019417\n\n\nInterpretation\n\nage: For every additional year of age, the expected number of calls increases by about 3%, holding subscription type constant. It is statistically significant.\nis_premium: Premium customers have about 41% fewer calls than regular customers, holding age constant (since 0.59 &lt; 1, it’s a decrease). It is statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Non-linear Regression</span>"
    ]
  }
]